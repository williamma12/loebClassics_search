{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "import os.path\n",
    "import re\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Checks if the response is HTML\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    '''\n",
    "    Gets the html data of the url\n",
    "    '''\n",
    "    try:\n",
    "        with closing(requests.get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "    \n",
    "def is_tag(obj):\n",
    "    '''\n",
    "    Checks if obj is a beautiful soup tag object\n",
    "    '''\n",
    "    return type(obj) == type(BeautifulSoup('<b>Test Tag</b>', 'html.parser').b)\n",
    "    \n",
    "\n",
    "def get_citation_data(url, save=True, data_path='data/'):\n",
    "    '''Get citation data of the book\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    url : str\n",
    "        String containing url of the first page of the book\n",
    "    save : boolean\n",
    "        save the visited pages to drive\n",
    "    data_path : string\n",
    "        path to save the pages and where the saved data is\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing citation data in key-value pairs\n",
    "    \n",
    "    '''\n",
    "    citation_data = dict()\n",
    "    \n",
    "    response = get_html(url)\n",
    "    if response is None:\n",
    "        raise ValueError(\"Bad book URL\")\n",
    "    \n",
    "    purchase_url = None\n",
    "    html = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    # Get DOI number of work\n",
    "    doi_data = html.findAll('div', {'class': 'doi'})\n",
    "    if doi_data is None:\n",
    "        raise RequestException(\"Could not find doi data\")\n",
    "    for datum in doi_data:\n",
    "        if is_tag(datum):\n",
    "            text = datum.get_text()\n",
    "            if 'DOI' in text:\n",
    "                citation_data['DOI'] = text.split(\" \")[1]\n",
    "    \n",
    "    # Get work title\n",
    "    title_data = html.findAll('span', {'class': 'workTitle'})\n",
    "    if title_data is None:\n",
    "        raise RequestException(\"Could not find title data\")\n",
    "    for datum in title_data:\n",
    "        if is_tag(datum):\n",
    "            citation_data['Title'] = datum.get_text()\n",
    "            \n",
    "    # Get volume number\n",
    "    volume_data = html.find_all('div', {'class': 'volumeLoc'})\n",
    "    if volume_data is None:\n",
    "        raise RequestException(\"Could not find volume data\")\n",
    "    for datum in volume_data:\n",
    "        if is_tag(datum) and datum.name == 'div':\n",
    "            for div_child in datum.children:\n",
    "                if is_tag(div_child) and div_child.name == 'h2':\n",
    "                    for child in div_child.children:\n",
    "                        if is_tag(child) and child.name == 'a':\n",
    "                            citation_data['Volume'] = child.get_text()\n",
    "        \n",
    "    # Get the url for the print edition for more of the citation data\n",
    "    for link in html.find_all('a'):\n",
    "        if link.get_text() == \"View cloth edition\":\n",
    "            purchase_url = link.get('href')\n",
    "    if purchase_url is None:\n",
    "        raise RequestException(\"Could not find cloth edition url\")\n",
    "    purchase_response = get_html(purchase_url)\n",
    "    if purchase_response is None:\n",
    "        raise ValueError(\"Bad purchase url\")\n",
    "    purchase_html = BeautifulSoup(purchase_response, 'html.parser')\n",
    "    \n",
    "    # Get the author and translator data\n",
    "    authors = purchase_html.find(id='authorList')\n",
    "    if authors is None:\n",
    "        raise RequestException(\"Missing authors list\")\n",
    "    for author in authors:\n",
    "        if is_tag(author):\n",
    "            text = author.get_text()\n",
    "            key = ''\n",
    "            value = ''\n",
    "            if \"by\" in text:\n",
    "                by_found = False\n",
    "                for word in text.split(\" \"):\n",
    "                    if by_found:\n",
    "                        value += word + \" \"\n",
    "                    else: \n",
    "                        key += word + ' '\n",
    "                    if word == \"by\":\n",
    "                        by_found = True\n",
    "            else:\n",
    "                key = 'Author'\n",
    "                value = text  \n",
    "            citation_data[key.strip()] = value.strip()\n",
    "    \n",
    "    # Get remaining book meta data (currently only ISBN and publication date)\n",
    "    book_data = purchase_html.find(id='bookMeta')\n",
    "    if book_data is None:\n",
    "        raise RequestException(\"Missing book data\")\n",
    "    for datum in book_data:\n",
    "        if is_tag(datum):\n",
    "            text = datum.get_text()\n",
    "            if 'ISBN' in text:\n",
    "                citation_data['ISBN'] = text.split(\" \")[1]\n",
    "            elif 'Publication' in text:\n",
    "                citation_data['Date'] = \" \".join(text.split(\" \")[1:])\n",
    "        \n",
    "    return citation_data\n",
    "\n",
    "\n",
    "def is_english(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def search_book(words, url, num_pages=999999999, save=True, data_path='data/'):\n",
    "    '''Get the surrounding lines around each appearence of a word in the words list\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        url of the book\n",
    "    words : list\n",
    "        list containing desired words\n",
    "    num_pages : int\n",
    "        number of pages to look at or until end of book\n",
    "    save : boolean\n",
    "        save the visited pages to drive\n",
    "    data_path : string\n",
    "        path to save the pages and where the saved data is\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe containing all appearences of the word in its surrounding paragraph\n",
    "    '''\n",
    "    citation_data = get_citation_data(url)\n",
    "    results = list()\n",
    "    \n",
    "    # Generate print url with page number iteration\n",
    "    link = list()\n",
    "    page = 0\n",
    "    prev_part = str()\n",
    "    for url_part in url.split(\".\"):\n",
    "        three_chars = list(url_part)[0:3]\n",
    "        if ''.join(three_chars) == 'xml':\n",
    "            page = int(prev_part)\n",
    "            link.append(\"{}\")\n",
    "        else:\n",
    "            link.append(prev_part)\n",
    "        prev_part = url_part\n",
    "    link.append(prev_part)\n",
    "    link = \".\".join(link[1:])\n",
    "    \n",
    "    # Iterate through pages\n",
    "    count = 0\n",
    "    while count < num_pages:\n",
    "        page_link = link.format(page) + \"&print\"\n",
    "        file_path = data_path+citation_data['Volume']+\"_\"+str(page)+\".html\"\n",
    "        \n",
    "        # Check if saved locally\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                response = file.read()\n",
    "            if response is None:\n",
    "                break;\n",
    "            html = BeautifulSoup(response, 'html.parser')\n",
    "        else:\n",
    "            response = get_html(page_link)\n",
    "            if response is None:\n",
    "                break;\n",
    "            html = BeautifulSoup(response, 'html.parser')\n",
    "            if save:\n",
    "                with open(file_path, 'w+') as file:\n",
    "                    file.write(str(html))\n",
    "        \n",
    "        # Make sure the page exists\n",
    "        try:\n",
    "            for div in html.findAll('h1', {'class': 't-display-1', 'id': 'pagetitle'}):\n",
    "                if is_tag(div) and div.get_text() == \"Page not found\":\n",
    "                    break;\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Search page for word\n",
    "        for section in html.findAll('section', {'class': 'div2'}):\n",
    "            if is_tag(section):\n",
    "                for child in section.children:\n",
    "                    if is_tag(child) and child.name == 'p':\n",
    "                        text = child.get_text().replace(\"\\n\", \" \")\n",
    "                        for word in words:\n",
    "                            if '{}'.format(word) in text:\n",
    "                                if is_english(word):\n",
    "                                    results.append([page, word, text])\n",
    "                                else:\n",
    "                                    results.append([page-1, word, text])\n",
    "        page += 2\n",
    "        count += 1\n",
    "    \n",
    "    citation_df = pd.DataFrame().append(pd.Series(citation_data), ignore_index=True)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = ['Page Number', 'Word', 'Paragraph']\n",
    "    citation_df['key'] = 1\n",
    "    results['key'] = 1\n",
    "    \n",
    "    results = pd.merge(citation_df, results, on='key').drop(['key'], axis=1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Date</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Translated by</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Word</th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Achilles Tatius</td>\n",
       "      <td>10.4159/DLCL.achilles_tatius-leucippe_clitopho...</td>\n",
       "      <td>January 1969</td>\n",
       "      <td>9780674990500</td>\n",
       "      <td>Leucippe and Clitophon</td>\n",
       "      <td>S. Gaselee</td>\n",
       "      <td>LCL 45</td>\n",
       "      <td>2</td>\n",
       "      <td>ἔθυον</td>\n",
       "      <td>Ἐνταῦθα ἥκων ἐκ πολλοῦ χειμῶνος, σῶστρα ἔθυον ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Achilles Tatius</td>\n",
       "      <td>10.4159/DLCL.achilles_tatius-leucippe_clitopho...</td>\n",
       "      <td>January 1969</td>\n",
       "      <td>9780674990500</td>\n",
       "      <td>Leucippe and Clitophon</td>\n",
       "      <td>S. Gaselee</td>\n",
       "      <td>LCL 45</td>\n",
       "      <td>3</td>\n",
       "      <td>Assyrian</td>\n",
       "      <td>1. Sidon is on the sea-board of the Assyrian O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Author                                                DOI  \\\n",
       "0  Achilles Tatius  10.4159/DLCL.achilles_tatius-leucippe_clitopho...   \n",
       "1  Achilles Tatius  10.4159/DLCL.achilles_tatius-leucippe_clitopho...   \n",
       "\n",
       "           Date           ISBN                   Title Translated by  Volume  \\\n",
       "0  January 1969  9780674990500  Leucippe and Clitophon    S. Gaselee  LCL 45   \n",
       "1  January 1969  9780674990500  Leucippe and Clitophon    S. Gaselee  LCL 45   \n",
       "\n",
       "   Page Number      Word                                          Paragraph  \n",
       "0            2     ἔθυον  Ἐνταῦθα ἥκων ἐκ πολλοῦ χειμῶνος, σῶστρα ἔθυον ...  \n",
       "1            3  Assyrian  1. Sidon is on the sea-board of the Assyrian O...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = search_book(words=['ἔθυον', 'Assyrian'], \n",
    "                             url=\"https://www.loebclassics.com/view/achilles_tatius-leucippe_clitophon/1969/pb_LCL045.3.xml?result=1&rskey=BWv00J\",\n",
    "                             num_pages=1,\n",
    "                            save=True)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for volume in search_results['Volume'].unique():\n",
    "    mask = search_results['Volume'] == volume\n",
    "    authors = search_results[mask]['Author'].unique()\n",
    "    titles = search_results[mask]['Title'].unique()\n",
    "    words = search_results[mask]['Word'].unique()\n",
    "    file_name = \"{}-{}-{}-{}.csv\".format(\"_\".join(authors), \"_\".join(titles), volume, \"_\".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results.to_csv(\"results/\"+file_name+\".csv\", sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
